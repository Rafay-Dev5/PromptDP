{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/lxuechen/private-transformers.git"
      ],
      "metadata": {
        "id": "8k-n6CzISB4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUeRXImEoN2Z"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch datasets evaluate accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "G4R3sgUyOfRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load pre-trained TinyBERT model and tokenizer\n",
        "model_name = \"prajjwal1/bert-tiny\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "fullChoice = int(input(\"Enter 1 for Full Fine-Tuning and 0 for Fine-Tuning with a single layer on top of it: \"))\n",
        "\n",
        "if not fullChoice:\n",
        "  for param in model.parameters():\n",
        "     param.requires_grad = False\n",
        "\n",
        "  model.classifier = nn.Linear(model.bert.pooler.dense.out_features, 2)\n",
        "\n",
        "print(f\"Number of Trainable Parameters= {sum(p.numel() for p in model.parameters() if p.requires_grad==True)}\")"
      ],
      "metadata": {
        "id": "MQRoZk7WMCVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TinyBert is loaded along with its tokenizer. Depending on user input, all layers are frozen and a layer is added on top of a LLM or full fine tuning is performed"
      ],
      "metadata": {
        "id": "s9j9bbHL09zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    return tokenizer(data[\"sentence\"], truncation=True, padding=\"max_length\",max_length=128)"
      ],
      "metadata": {
        "id": "z6cpIIa0Z5gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cleaned_data(task):\n",
        "    if task == 1:\n",
        "        print(\"SST2 Dataset\")\n",
        "\n",
        "        task = \"sst2\"\n",
        "\n",
        "        # Load SST-2 dataset\n",
        "        dataset = load_dataset(\"glue\", task)\n",
        "        tokenized_data = dataset.map(preprocess_data, batched=True)\n",
        "        tokenized_data = tokenized_data.remove_columns([\"idx\",\"sentence\"])\n",
        "        tokenized_data = tokenized_data.rename_column(\"label\", \"labels\")\n",
        "        tokenized_data.set_format(\"torch\")\n",
        "        return tokenized_data, task\n",
        "\n",
        "\n",
        "\n",
        "    elif task == 2:\n",
        "        print(\"QNLI Dataset\")\n",
        "    elif task == 3:\n",
        "        print(\"MNLI Dataset\")\n",
        "    elif task == 4:\n",
        "        print(\"QQP Dataset\")\n",
        "    else:\n",
        "        print(\"Invalid Dataset\")\n",
        "        task = None\n"
      ],
      "metadata": {
        "id": "3PBr76QEZuFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taskChoice = int(input(\"Enter \\n1 for SST2, 2 for QNLI, 3 for MNLI and 4 for QQP: \"))\n",
        "\n",
        "tokenized_data , task = load_cleaned_data(taskChoice)\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_data['train'], shuffle=True, batch_size=1024)\n",
        "val_dataloader = DataLoader(tokenized_data['validation'], shuffle=True, batch_size=1024)"
      ],
      "metadata": {
        "id": "wDoCGrKcocva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is tokenized, cleaned and separated into training and validation sets"
      ],
      "metadata": {
        "id": "a-HaYxEf1kDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dataloader) * epochs),)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "BUVGIDXXaZpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def evaluate_model(model,dataloader,task):\n",
        "    metric = evaluate.load(\"glue\", task)\n",
        "    model.eval()\n",
        "    for batch in dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "    return metric.compute()"
      ],
      "metadata": {
        "id": "w0Um4n5Ghqfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Without Differential Privacy"
      ],
      "metadata": {
        "id": "ihG4bW052wyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(model,optimizer,train_dataloader,val_dataloader,loss_fn,lr_scheduler,tqdm,task,epochs=5 ,dp=False):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for step,batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "            # Forward pass\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            if dp:\n",
        "              loss = F.cross_entropy(outputs.logits, batch[\"labels\"]).mean(dim=0).unsqueeze(0)\n",
        "\n",
        "              # Backward pass and update with DP\n",
        "              optimizer.step(loss=loss)\n",
        "\n",
        "            else:\n",
        "              loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
        "              total_loss += loss.detach().float()\n",
        "\n",
        "              # Backward pass and update without DP\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if not dp:\n",
        "            train_epoch_loss = total_loss / len(train_dataloader)\n",
        "            train_ppl = torch.exp(train_epoch_loss)\n",
        "            print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} \")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        with torch.no_grad():\n",
        "            val_accuracy = evaluate_model(model, val_dataloader,task)\n",
        "            print(f\"Epoch {epoch+1}, Validation Accuracy {'with' if dp else 'without'} DP: {val_accuracy}\")\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "d5Hj8GvoazIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainModel(model,optimizer,train_dataloader,val_dataloader,loss_fn,lr_scheduler,tqdm,task)"
      ],
      "metadata": {
        "id": "dpfmqLr6cx1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###With Differential Privacy\n"
      ],
      "metadata": {
        "id": "tMcIBhvr2gYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, torch\n",
        "from private_transformers import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    model,\n",
        "    batch_size=1024,\n",
        "    sample_size=tokenized_data['train'].num_rows,\n",
        "    epochs=5,\n",
        "    max_grad_norm=0.2,\n",
        "    target_epsilon=3,\n",
        "    clipping_mode=\"ghost\"\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.01)\n",
        "privacy_engine.attach(optimizer)\n",
        "\n",
        "for epoch in range(5):\n",
        "        model.train()\n",
        "        # total_loss = 0\n",
        "        for step,batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            loss = F.cross_entropy(outputs.logits, batch[\"labels\"]).mean(dim=0).unsqueeze(0)\n",
        "            # Backward pass and update with DP\n",
        "            optimizer.step(loss=loss)\n",
        "\n",
        "            lr_scheduler.step()\n",
        "\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        with torch.no_grad():\n",
        "            val_accuracy = evaluate_model(model, val_dataloader,task)\n",
        "            print(f\"Epoch {epoch+1}, Validation Accuracy DP: {val_accuracy}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "nugevEScyb_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}