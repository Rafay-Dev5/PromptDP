{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxuLvXn5ByWd"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch datasets peft evaluate opacus accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_config,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftType,\n",
        "    LoraConfig,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "from torch import nn\n",
        "\n",
        "from opacus.privacy_engine import PrivacyEngine\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_name_or_path = \"prajjwal1/bert-tiny\"\n",
        "num_epochs = 5\n",
        "lr = 0.01\n",
        "batch_size = 1024"
      ],
      "metadata": {
        "id": "l4j9XOTjEqN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"right\")\n",
        "if getattr(tokenizer, \"pad_token_id\") is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    outputs = tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "IKmz2PljHAZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cleaned_data(task):\n",
        "    if task == 1:\n",
        "        print(\"SST2 Dataset\")\n",
        "\n",
        "        task = \"sst2\"\n",
        "\n",
        "        # Load SST-2 dataset\n",
        "        dataset = load_dataset(\"glue\", task)\n",
        "        tokenized_data = dataset.map(preprocess_data, batched=True)\n",
        "        tokenized_data = tokenized_data.remove_columns([\"idx\",\"sentence\"])\n",
        "        tokenized_data = tokenized_data.rename_column(\"label\", \"labels\")\n",
        "        tokenized_data.set_format(\"torch\")\n",
        "        return tokenized_data\n",
        "\n",
        "\n",
        "\n",
        "    elif task == 2:\n",
        "        print(\"QNLI Dataset\")\n",
        "    elif task == 3:\n",
        "        print(\"MNLI Dataset\")\n",
        "    elif task == 4:\n",
        "        print(\"QQP Dataset\")\n",
        "    else:\n",
        "        print(\"Invalid Dataset\")\n",
        "        task = None\n"
      ],
      "metadata": {
        "id": "A0lAbe5zjZuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taskChoice = int(input(\"Enter \\n1 for SST2, 2 for QNLI, 3 for MNLI and 4 for QQP: \"))\n",
        "\n",
        "tokenized_data = load_cleaned_data(taskChoice)\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_data['train'], shuffle=True, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(tokenized_data['validation'], shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "VzBPEzhdkWpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is tokenized, cleaned and separated into training and validation"
      ],
      "metadata": {
        "id": "8bFkbpAa3YJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(task_type=\"SEQ_CLS\",  r=8, lora_alpha=32, lora_dropout=0.1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
        "lora_model = get_peft_model(model, peft_config)\n",
        "lora_dp_model = get_peft_model(model, peft_config)\n",
        "\n",
        "lora_model.print_trainable_parameters()\n",
        "lora_dp_model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "z9CU3fK_HMye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA model is configured via Hugging Face PEFT API. Its hyperparamters have been selected as given in the PEFT blog"
      ],
      "metadata": {
        "id": "72V0E2Qd4cID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = SGD(lora_model.parameters(), lr=lr)\n",
        "dp_optimizer = SGD(lora_dp_model.parameters(),lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dataloader) * num_epochs),)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "k0xllzl2HdSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,eval_dataloader,task):\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "    return metric.compute()"
      ],
      "metadata": {
        "id": "COBj2u5oISSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Without Differential Privacy"
      ],
      "metadata": {
        "id": "FGwFA_tY5Kno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(model,optimizer,train_dataloader,val_dataloader,loss_fn,lr_scheduler,tqdm,task,epochs=5,dp=False):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for step,batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "            # Forward pass\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
        "\n",
        "            # Backward pass and update without DP\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        with torch.no_grad():\n",
        "            val_accuracy = evaluate_model(model, val_dataloader,task)\n",
        "            print(f\"Epoch {epoch+1}, Validation Accuracy {'with' if dp else 'without'} DP: {val_accuracy}\")\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "b76pzA2toDtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainModel(lora_model,optimizer,train_dataloader,val_dataloader,loss_fn,lr_scheduler,tqdm,\"sst2\")"
      ],
      "metadata": {
        "id": "M7hixVLwoHpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###With Differential Privacy"
      ],
      "metadata": {
        "id": "NZTbHOV35np6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "lora_dp_model,dp_optimizer,dataloader = privacy_engine.make_private_with_epsilon(\n",
        "    module=lora_dp_model,\n",
        "    optimizer=dp_optimizer,\n",
        "    data_loader=train_dataloader,\n",
        "    target_epsilon = 3,\n",
        "    target_delta = 1/tokenized_data['train'].num_rows,\n",
        "    epochs = 5,\n",
        "    max_grad_norm=0.2,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "m6XV8f7dJcbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "for epoch in range(5):\n",
        "    lora_dp_model.train()\n",
        "    losses = []\n",
        "    for step,batch in enumerate(tqdm(dataloader)):\n",
        "\n",
        "        dp_optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = lora_dp_model(**batch)\n",
        "        loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
        "        # print(loss.item())\n",
        "        # losses.append(loss.item())\n",
        "\n",
        "        # Backward pass and update\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        dp_optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    # train_epoch_loss = np.mean(losses)\n",
        "\n",
        "    # print(f\"{epoch=}: {train_epoch_loss=} \")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    with torch.no_grad():\n",
        "        val_accuracy = evaluate_model(lora_dp_model, val_dataloader,\"sst2\")\n",
        "        print(f\"Epoch {epoch+1}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "print(\"Training complete\")"
      ],
      "metadata": {
        "id": "L7FjsCg1PBXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}