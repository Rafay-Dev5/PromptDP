{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch datasets peft accelerate evaluate -U\n"
      ],
      "metadata": {
        "id": "agwaV5ffGZws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG91ONafFwcw"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_config,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftType,\n",
        "    PromptEncoderConfig,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "model_name_or_path = \"prajjwal1/bert-tiny\"\n",
        "num_epochs = 5\n",
        "lr = 0.01\n",
        "batch_size = 1024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_dataset(\"glue\",\"sst2\")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"sst2\")"
      ],
      "metadata": {
        "id": "XYAU2tV5GJ-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"right\")\n",
        "if getattr(tokenizer, \"pad_token_id\") is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # max_length=None => use the model max length (it's actually the default)\n",
        "    outputs = tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "sahPxA9uGrep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\",\"sentence\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")"
      ],
      "metadata": {
        "id": "06AQRACLHK7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=10, encoder_hidden_size=128)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ri-oXOENIPOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_model = get_peft_model(model, peft_config)\n",
        "p_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "XaQ61uZvvXyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft Prompt model has been configured via hugging face API"
      ],
      "metadata": {
        "id": "RWHzQv2Q88d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PrefixTuningConfig, get_peft_model\n",
        "\n",
        "peft_config = PrefixTuningConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=10)\n",
        "prefix_model = get_peft_model(model, peft_config)\n",
        "prefix_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "IaaPlv6EIyM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prefix model has been configured via hugging face API"
      ],
      "metadata": {
        "id": "AaRN4XWb9GcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch.optim import SGD\n",
        "from torch import nn\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=1024)\n",
        "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], shuffle=True, batch_size=1024)\n",
        "# Define optimizer and loss function\n",
        "optimizer = SGD(prefix_model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dataloader) * 5),)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "prefix_model.to(device)"
      ],
      "metadata": {
        "id": "t3ME3U28H-uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,eval_dataloader,task):\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "    return metric.compute()"
      ],
      "metadata": {
        "id": "fYicS97L2HS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Without Differential Privacy"
      ],
      "metadata": {
        "id": "Y7rlOCHF8Jea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    prefix_model.train()\n",
        "    total_loss = 0\n",
        "    for step,batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # Forward pass\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = prefix_model(**batch)\n",
        "        loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
        "        total_loss += loss.detach().float()\n",
        "\n",
        "\n",
        "        # Backward pass and update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    train_epoch_loss = total_loss / len(train_dataloader)\n",
        "    train_ppl = torch.exp(train_epoch_loss)\n",
        "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} \")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    with torch.no_grad():\n",
        "        val_accuracy = evaluate_model(prefix_model, val_dataloader,\"sst2\")\n",
        "        print(f\"Epoch {epoch+1}, Validation Accuracy  DP: {val_accuracy}\")\n",
        "\n",
        "\n",
        "print(\"Training complete\")\n"
      ],
      "metadata": {
        "id": "vWd67AbCMsDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###With Differential Privacy"
      ],
      "metadata": {
        "id": "AcrdPZ6o8C-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft prompt parameters have been assumed as all the possible trainable parameters and hence only trainable parameters' gradients has been checked before updating them"
      ],
      "metadata": {
        "id": "_Ly9ml5r9goq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "noise_scale = 0.157\n",
        "sampling_rate = 0.15\n",
        "max_gradient_norm = 0.2\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(5):\n",
        "    prefix_model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "\n",
        "        # Sample mini-batch according to sampling rate\n",
        "        if np.random.rand() <= sampling_rate:\n",
        "            # inputs = tokenizer(batch[\"sentence\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True,max_length=128)\n",
        "            # inputs = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "            # inputs = tokenized_datasets.remove_columns([\"idx\",\"sentence\"])\n",
        "            # inputs.set_format(\"torch\")\n",
        "\n",
        "            # labels = batch[\"labels\"]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = prefix_model(**batch)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(logits, batch[\"labels\"])\n",
        "            total_loss += loss.detach().float()\n",
        "\n",
        "            # Compute gradients w.r.t. soft prompt parameters\n",
        "            loss.backward()\n",
        "\n",
        "            # Modify gradients for prompt-specific parameters\n",
        "            for name, param in prefix_model.named_parameters():\n",
        "                if param.requires_grad and param.grad != None:\n",
        "\n",
        "                    gradients = param.grad\n",
        "                    gradient_norm = torch.norm(gradients)\n",
        "                    if gradient_norm > max_gradient_norm:\n",
        "                        gradients = gradients * max_gradient_norm / gradient_norm\n",
        "\n",
        "                    # Add noise to gradients\n",
        "                    noise = torch.normal(mean=0, std=noise_scale, size=gradients.size())\n",
        "                    noisy_gradients = gradients + noise\n",
        "\n",
        "                    param.grad = noisy_gradients\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        lr_scheduler.step\n",
        "    train_epoch_loss = total_loss / len(train_dataloader)\n",
        "    # train_ppl = torch.exp(train_epoch_loss)\n",
        "    print(f\"{epoch=}: {train_epoch_loss=} \")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    with torch.no_grad():\n",
        "        val_accuracy = evaluate_model(prefix_model, val_dataloader,\"sst2\")\n",
        "        print(f\"Epoch {epoch+1}, Validation Accuracy  DP: {val_accuracy}\")\n"
      ],
      "metadata": {
        "id": "K5lK10uvNIi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Privacy Cost Calculation"
      ],
      "metadata": {
        "id": "wnPLLAl06kSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def calc_epsilon(delta, noise_scale, iterations, sampling_rate):\n",
        "  epsilon = noise_scale * math.sqrt(2 * math.log(1 / delta)) / math.sqrt(sampling_rate * iterations)\n",
        "  return epsilon,delta\n",
        "\n",
        "\n",
        "epsilon, delta = calc_epsilon(sampling_rate=0.15, delta=1/67349, noise_scale=0.157, iterations=5)\n",
        "print(\"Privacy cost: (epsilon, delta) = ({}, {})\".format(epsilon, delta))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nPs0-vbYQVg",
        "outputId": "46759cce-7c96-4231-a2b0-e6a52cbc0324"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy cost: (epsilon, delta) = (0.8548509274140579, 1.4848030408766277e-05)\n"
          ]
        }
      ]
    }
  ]
}